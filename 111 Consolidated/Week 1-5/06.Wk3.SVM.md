# SVM

# Concept
![alt text](image-29.png)


# Components
> - margin is broader (more robust)
![alt text](image-30.png)
![alt text](image-31.png)

# Math
![alt text](image-32.png)
---
> COST = Training Cost + \lambda penalty

> - **Training Cost = Hinge Loss**
![alt text](image-33.png)
**Example**
![alt text](image-34.png)
![alt text](image-35.png)

> - **Regularization term**
 ![alt text](image-36.png)

> **Overall Math**
![alt text](image-37.png)

# Stochastic Gradient descend 
> Why? Minimize cost function
![alt text](image-38.png)
> - rescale for unit variance
> - G(i) - maximization term
> - G(0) - reguralixation term

![alt text](image-39.png)

**Concept**
- iterative process
- each step si U(n).next step is U(n+1)
- aeta => small step size
- if new step smaller than previous step, we update U(n+1)
- p => Direction

## Epoch
> **NOTE:** EPOCH : Instead of full dataset size, we use batch size (denoted by "b")
> ![alt text](image-40.png)

**Seasons** 
 - smaller batches than epochs
 - ![alt text](image-41.png)

**Gradient**
![alt text](image-42.png)

# Pros and Cons
## pros
> - Easy to train
> - Fast classification

## cons
> - Need enough features
