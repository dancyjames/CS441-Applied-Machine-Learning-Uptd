# Linear regression

**vs. Classification:**
- classification is small set and output is **categorical**
- regression **predicts "numeric value"**
- "Classification" can be seen as subset of "Regression" where each class is +1 or -1.

# Overview
![alt text](image-43.png)

# Characteristics of regression
> value of y can be different for the same feature value
Due to
- randomness /Noise
- **Y is not function of x**(y is subjective values) (same item, different store, different prices)
  
# Practical representation
![alt text](image-44.png)
- minimise "he"- sign for error 

# Finding Beta
![alt text](image-45.png)

> - Both direct and probabilistic 
> - Both gives one solution. minimize mean square error to find beta
![alt text](image-46.png)

# beta - Math
![alt text](image-47.png)
> Note :
> - This is direct if beta has full rank
> What is FULL RANK?
> 	*Full rank refers to a matrix that has linearly independent columns. In regression, this is important because if your design matrix X (which includes the features) has linearly dependent columns (co-linearity or multicollinearity), it can lead to problems when trying to compute the inverse of X^T X (which appears in the solution for ordinary least squares regression).*
> ![alt text](image-48.png)
> Moore - Penrose:
> - ![alt text](image-49.png)

# Performance Measurement for Regression
![alt text](image-50.png)

# Cons of regression
- Outlier will increase MSE and will exert force on Beta to move towards the direction of Outlier

# Two Methods to deal with Outliers

1. **Leverage and Hat Matrix**

- Hat is everything that is not "y"
- ![alt text](image-51.png)
- The diagonal h(ii) is the "Leverage"
- sum of square element of each row <=1
- If "leverage" is big, then it indicates "Outlier" ashtr sum of the row is <=1. which means other values are small.

> NOTE: Another use of Hat Matrix is to standarize **residues**
> - ![alt text](image-52.png)
> - This aims at (1) reduce error (2) within one standard deviation

2. **Cook' distance**
 ![alt text](image-53.png)

 # Linear regression transformation
 ## Why transformation?
 ![alt text](image-54.png)
 > -LR that has low R2 can be improved with transformation
> - plotting is an option to see the fit (if one explanatory variable). To find if polynomial or log will fit better

## Two options for transformation
**1. BOX COX Transformation**
![alt text](image-55.png)

**2. Polynomial**
this is when the independant variable is dependant on "higher degree value' of independant variable

**like polynomial**
![alt text](image-56.png)

# Regularization
## Why?
- When two explanatory are co-related, then the "xTx" can have small eigen values.

- what that means is xTxBeta is also the apprxomimately same as betahat + some unknown number.

- this can cause beta_hat to be huge.

### Two types of regularization
1. Ridge /L2 Norm
 ![alt text](image-57.png)

 - doesnt remove features.
 - Regularization term that penalized large value of beta
2. Lasso/L1 Norm

-- more on this later.


---
# The end
