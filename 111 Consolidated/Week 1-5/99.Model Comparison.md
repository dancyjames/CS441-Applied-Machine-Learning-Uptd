
# Leave one and K-fold

![alt text](image-8.png)

| Leave one out    | K-fold |
| -------- | ------- |
| use almost all data for training each time(high variance) | use less data for training in each iteration(still good performance) |
| computational expensive as model need to train N times |efficient than LOO as it reduces number of training runs to "k" and k < N|

# KNN - pros and cons
![alt text](image-10.png)

# KNN: pros and cons of KD tree and hashing
![alt text](image-13.png)

# Nearest Neighbour vs. Naive Bayes
| KNN  | Naive Bayes|
| -------- | ------- |
| Missing data cannot be handled | robust to missing data |
| Not good for high dimensional data | Good for high dimensional data |
| too sensitive to distribution of data | not sensitive |

# Naive Bayes - Adv and Disadv
![alt text](image-14.png)