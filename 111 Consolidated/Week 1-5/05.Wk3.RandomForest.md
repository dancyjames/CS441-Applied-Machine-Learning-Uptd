# Decision tress

# Concept
![alt text](image-16.png)

> - It is a "If-then" process
> - It is recursive.
> - Once algorithm moved forward, it doesnt back track to re-consider the earlier choices
> ![alt text](image-17.png)

# Determine Best Split
## Logic
> - more than one way to split 
> - based on feature. find subset that has more information gain

> **Information Gain**
- Information gain is **higher** when classes have memebers from one class and less when it is diverse
- Preferred - **more** Information gain

![alt text](image-18.png)
> - bottom has higher Information gain

> **Entropy**
- Entropy is low, Informaion gain is high
- **Sample**: 100 elements

![alt text](image-23.png)

## Entropy Math
![alt text](image-24.png)
![alt text](image-25.png)

## Math - Information Gain 
![alt text](image-26.png)
![alt text](image-27.png)

# Splits
- if features can eb sorted, we can use sorted splits, like binary sort. But, it is tough for sorting by more than one features
- Threshold can be used to split

# Pros and Cons of Decsion trees
**Pros**
![alt text](image-19.png)
**Cons**
![alt text](image-20.png)

---

# Random Foreest
## Concept
![alt text](image-21.png)

## Strategies
1. **Simple**: using random splits, train and test data split
2. **Bagging**
![alt text](image-22.png)

> When item is missing a feature: Use Mode or Median to associate value of that feature from other examples