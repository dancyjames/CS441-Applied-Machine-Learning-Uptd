# Agenda
![alt text](image-1.png)

# Abt. explanatory variables
![alt text](image-2.png)
> - we need to identify required explanatory variables from the subset
> 3 kind of errors
> - Bias - underfitting - less variables - simple model and doesnt reflect the data
> - variance - complex models - more variables and specific - not generalize enough - overfitting

# Bias and variance
![alt text](image-3.png)
> - noise is independant of "x"
> - noise - mean of error and variance of sigma squared
> - fhat -> predicion
> "All possible data" - just not on training data
 
![alt text](image-4.png)
> - variance of y is same as noise = sigma squared
>  3 components
> 1. variance of fhat => variance
> 2. bias is (f -Expected value of fhat SQUARED)
> 3. sigma squared => irreducible error

![alt text](image-5.png)

> - explained earlier...

# Choosing model
![alt text](image-6.png)
> - penalize the number of parameters by adding to training error
> - we already discussed hinge loss (we have another regularization, lasso)
> - both tools use cross validation (information criteria and regularization)


# AIC , BIC (Akaike and Bayes Information Criteria)
![alt text](image-7.png)

> - "k" parameers
> -"Log likelihood"
> - "AIC MIGHT HAVE MORE PARAMETERS ESTIMATED, But has strong theory"
> - "N" for BIC is size of dataset
> - "d" - number of co-eff
> - usually has statiscial methods to calcualte AIC and BIC.


# Cross- validation
![alt text](image-8.png)

> - both becomes less when lamdda =1
> - use cross validation

# Exploration of parameters
![alt text](image-9.png)

> - to choose the variables, two greedy strategies
> - forward - starts with empty set

# Stagewise regression
![alt text](image-10.png)
- backward - starts with all variables and removes the variables

# variable importance
![alt text](image-11.png)

- if same variable in  "km" vs. "meters" , its significance changes (unit of measurement)
- even co-eff  is zero, it still might have an impact


---
# The end